defaults:
  - base
  - callbacks: [early_stopping, model_checkpoint, model_summary, char_module_writer, progress_bar]
  - datamodule: char
  - logger: null
  - encoder: char_deberta_tiny
  - module: char
  - optimizer: adamw
  - scheduler: constant_schedule_with_warmup
  - trainer: debug
  - _self_

max_seq_length: 256
do_predict_after_train: true
checkpoint_path: ""
document_split_stride: -1

# For word normalization
denormalize_probability: 0.5

# set monitor and mode for early_stopping and model_checkpoint
monitor: valid/aggregated_char_metrics
mode: max
aggregating_metrics:
  - word_segmentation_f1
#  - word_normalization_f1  # exclude because this metric is unstable

# hyper-parameters to be tuned
lr: 2e-5
warmup_steps: 100
effective_batch_size: 16

# environment dependent settings
devices: ${oc.env:GPUS,0}
max_batches_per_device: ${oc.env:MAX_BATCHES_PER_DEVICE,2}
num_workers: ${oc.env:NUM_WORKERS,0}

ignore_hparams_on_save: false

# constants
hparams_to_ignore_on_save: []
