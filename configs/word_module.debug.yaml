defaults:
  - base
  - callbacks: [early_stopping, model_checkpoint, model_summary, word_module_writer, progress_bar]
  - datamodule: word
  - logger: null
  - encoder: word_deberta_tiny
  - module: word
  - optimizer: adamw
  - scheduler: constant_schedule_with_warmup
  - trainer: debug
  - _self_

max_seq_length: 128
do_predict_after_train: true
checkpoint_path: ""
document_split_stride: 2
training_tasks:
  - reading_prediction
  - morphological_analysis
  - word_feature_tagging
  - ner
  - base_phrase_feature_tagging
  - dependency_parsing
  - cohesion_analysis
  - discourse_parsing

# for dependency parser
dependency_topk: 4

# for discourse parser
discourse_parsing_threshold: 0.0

# for cohesion analysis
# pas_analysis, bridging_reference_resolution, and/or coreference_resolution
cohesion_tasks: [pas_analysis, bridging_reference_resolution, coreference_resolution]
exophora_referents: ["著者", "読者", "不特定:人", "不特定:物"]
restrict_cohesion_target: true
pas_cases: ["ガ", "ヲ", "ニ", "ガ２"]
br_cases: ["ノ"]

# set monitor and mode for early_stopping and model_checkpoint
monitor: valid/aggregated_word_metrics
mode: max
aggregating_metrics:
  - reading_prediction_accuracy
  - morphological_analysis_f1
  - macro_word_feature_tagging_f1
  - ner_f1
  - macro_base_phrase_feature_tagging_f1
  - base_phrase_LAS_f1
  - cohesion_analysis_f1
  - discourse_parsing_f1

# hyper-parameters to be tuned
lr: 1e-4
max_epochs: 2
warmup_steps: 10
effective_batch_size: 16

# environment dependent settings
devices: ${oc.env:DEVICES,1}
max_batches_per_device: ${oc.env:MAX_BATCHES_PER_DEVICE,2}
num_workers: ${oc.env:NUM_WORKERS,0}
compile: ${oc.env:COMPILE,false}

ignore_hparams_on_save: false

# constants
special_tokens: ["[著者]", "[読者]", "[不特定:人]", "[不特定:物]", "[NULL]", "[NA]", "[ROOT]"]
hparams_to_ignore_on_save: []
