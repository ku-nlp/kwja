defaults:
  - base
  - callbacks: [early_stopping, model_checkpoint, model_summary, word_module_writer, progress_bar]
  - datamodule: word
  - dataset: word
  - logger: wandb
  - model: word_roberta_large
  - module: word
  - optimizer: adamw
  - scheduler: constant_schedule_with_warmup
  - trainer: default
  - _self_

do_predict_after_train: true
checkpoint_path: ""
document_split_stride: 1
training_tasks: [
  "reading_prediction",
  "word_analysis",
  "ner",
  "word_feature_tagging",
  "base_phrase_feature_tagging",
  "dependency_parsing",
  "cohesion_analysis",
  "discourse_parsing",
]

# for dependency parser
k: 4

# for discourse parser
discourse_parsing_threshold: 0.0

# for cohesion analysis
pas_cases: ["ガ", "ヲ", "ニ", "ガ２"]
bar_rels: ["ノ"]
exophora_referents: ["著者", "読者", "不特定:人", "不特定:物"]
cohesion_tasks: [pas_analysis, bridging, coreference]  # pas_analysis, bridging, and/or coreference
restrict_cohesion_target: true

# set monitor and mode for early_stopping and model_checkpoint
monitor: valid/aggregated_word_metrics
mode: max
aggregating_metrics: [
  "reading_prediction_accuracy",
  "word_analysis_f1",
  "ner_f1",
  "macro_word_feature_f1",
  "macro_base_phrase_feature_f1",
  "base_phrase_LAS_f1",
  "cohesion_analysis_f1",
  "discourse_parsing_f1",
]

# hyper-parameters to be tuned
warmup_steps: 100
effective_batch_size: 16

# environment dependent settings
devices: ${oc.env:GPUS,0}
max_batches_per_device: 1
num_workers: 0

hparams_to_ignore_on_save:
  - project
  - work_dir
  - seed
  - name
  - exp_dir
  - run_id
  - run_dir
  - config_name
  - callbacks:
    - early_stopping
    - model_checkpoint
    - model_summary
    - progress_bar
  - datamodule:
    - train
    - valid
    - test
  - logger
  - hparams_to_ignore_on_save
